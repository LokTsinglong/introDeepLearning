{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a256ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#均方误差函数，mean squared error\n",
    "def mse(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1021e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "t=np.array(t)\n",
    "y=[0.1,0.2,0.6,0.1,0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6afffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.11000000000000003)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=mse(y,t)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13cb65ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2=np.array([0.1,0.6,0.2,0.1,0.0,0.0,0.0,0.0,0.0,0.0])\n",
    "loss2=mse(y2,t)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41a4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#交叉熵误差，cross entropy error\n",
    "def cross_entropy_error(y,t):\n",
    "    delta= 1e-7 #防止log(0)的情况,返回NaN\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6632019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.510825457099338)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss3=cross_entropy_error(y,t)\n",
    "loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b215d5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.6094374124342252)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss4=cross_entropy_error(y2,t)\n",
    "loss4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07698a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 64)\n",
      "(1197, 64)\n",
      "(600,)\n",
      "(1197,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "x_train, x_test = digits.data[:600],digits.data[600:]\n",
    "y_train,y_test = digits.target[:600],digits.target[600:]\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a74069b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n"
     ]
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size=10\n",
    "batch_mask=np.random.choice(train_size,batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]\n",
    "print(x_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cbf1298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55851, 37127, 22271, 25896, 44370, 54681, 33724, 36890, 53988,\n",
       "       35775], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50e47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        # 转化为2D\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+1e-7))/batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8b0bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果t不是one-hot编码，而是数字标签\n",
    "def cee(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t=t.reshape(1,t.size)\n",
    "        y=y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c4fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb4a675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "    #或者return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16f225da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4.0**2\n",
    "numerical_diff(function_tmp1,3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b5b24d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x1):\n",
    "    return x1**2 + 4.0**2\n",
    "numerical_diff(function_tmp1,4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34085d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现梯度\n",
    "def numerical_gradient(f,x):\n",
    "    h=1e-4\n",
    "    grad = np.zeros_like(x) #创建与x同形状的数组\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1-fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a122fff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dea1fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x=init_x\n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x-=lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37033672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39785867,  0.53047822])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2,init_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddcac269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.10452420e-11,  1.08060323e-10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#修改lr试试看\n",
    "def gradient_descent_2(f,init_x,lr=0.1,step_num=100):\n",
    "    x=init_x\n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x-=lr*grad\n",
    "    return x\n",
    "gradient_descent_2(function_2,init_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48035bc9",
   "metadata": {},
   "source": [
    "可以发现改变学习率，使得梯度下降的最后结果更加接近（0，0）这一个点\n",
    "下面可以看一下改变学习率对于最后梯度下降的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cce9015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.29863664e+13,  1.28237547e+13])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(function_2,init_x=init_x,lr=10.0,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72084615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.29863664e+13,  1.28237547e+13])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(function_2,init_x=init_x,lr=1e-10,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc15a44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_3(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "init_x=np.array([-3.,4.])\n",
    "def gradient_descend_3(f,init_x,lr,step_num):\n",
    "    x=init_x\n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x-=lr*grad\n",
    "    return x\n",
    "init_x=np.array([-3.,4])\n",
    "gradient_descend_3(function_3,init_x,lr=0.1,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89d4e451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34235971e+12, -3.96091057e+12])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descend_3(function_3,init_x,lr=10.0,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70ebfe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34235971e+12, -3.96091057e+12])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descend_3(function_3,init_x,lr=1e-10,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9d6e6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x=np.array([-3.,4])\n",
    "gradient_descend_3(function_3,init_x,lr=1e-10,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9440bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x=np.array([-3.,4])\n",
    "gradient_descend_3(function_3,init_x,lr=10.0,step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acb33daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    c=np.max(x)\n",
    "    exp_a= np.exp(x-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45ed7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta= 1e-7\n",
    "    if y.ndim==0:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_szie=y.shape[0]\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b38cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单神经网络\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #采用高斯分布/正态分布进行初始化\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    def loss(self,x,t):\n",
    "        z=self.predict(x)\n",
    "        y=softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a78ff714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2820929   0.93764713  0.52998645]\n",
      " [ 0.35327828 -0.73616693 -0.606531  ]]\n"
     ]
    }
   ],
   "source": [
    "net=simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a9e6c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45130528 -0.09996196 -0.22788603]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([0.6,0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd459a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5af4fe1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09492076249958346)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.array([0,1,0])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1d7c2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.1300550841909898)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.array([1,0,0])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d2b32",
   "metadata": {},
   "source": [
    "loss function损失函数，越小越好！越大越坏！ 坏坏~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f015e055",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(W):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m net.loss(x,t)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dW = \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(dW)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mnumerical_gradient\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m      4\u001b[39m grad = np.zeros_like(x) \u001b[38;5;66;03m#创建与x同形状的数组\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x.size):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tmp_val = \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m     x[idx] = tmp_val + h\n\u001b[32m      8\u001b[39m     fxh1 = f(x)\n",
      "\u001b[31mIndexError\u001b[39m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "dW = numerical_gradient(f,net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2ae46e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mnumerical_gradient\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m      4\u001b[39m grad = np.zeros_like(x) \u001b[38;5;66;03m#创建与x同形状的数组\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x.size):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tmp_val = \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m     x[idx] = tmp_val + h\n\u001b[32m      8\u001b[39m     fxh1 = f(x)\n",
      "\u001b[31mIndexError\u001b[39m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([[3.0,4.0],[2.,3.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88eaf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_edited(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=[\"multi_index\"],op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        original_value = x[idx]\n",
    "        x[idx] = original_value + h\n",
    "        fxh1=f(x)\n",
    "        x[idx] = original_value - h\n",
    "        fxh2=f(x)\n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = original_value\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65f391fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[31mTypeError\u001b[39m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnumerical_gradient_edited\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mnumerical_gradient_edited\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m     10\u001b[39m x[idx] = original_value - h\n\u001b[32m     11\u001b[39m fxh2=f(x)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m = (fxh1-fxh2)/(\u001b[32m2\u001b[39m*h)\n\u001b[32m     13\u001b[39m x[idx] = original_value\n\u001b[32m     14\u001b[39m it.iternext()\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "numerical_gradient_edited(function_2,np.array([[3.0,4.0],[2.,3.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f696d4",
   "metadata": {},
   "source": [
    "之前，numerical_gradient(function_2,np.array([[3.0,4.0],[2.,3.]]))拿这个做测试，是非常错误的!\n",
    "因为我们注意到function2: x[0]**2+x[1]**2,这个表达式只需要输入两个参数即可，但是我尝试输入一个形状为（2，2）的矩阵进去，书中给的例子是f(W): net.loss(x,t), 在loss里面用到的是cross_entropy_error: 具体的表达式：-np.sum(t*np.log(y+h))/batch_size，里面就是运用了np的一些运算，所以自然而然可以在输入的时候输入一些多维度的np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "140b4374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04365709,  0.02322284,  0.02043425],\n",
       "       [-0.06548563,  0.03483426,  0.03065137]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient_edited(f,net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7658f68",
   "metadata": {},
   "source": [
    "这时候跑通了，并且形状也是没有问题的，可以认为是numerical_gradient_edited修改之后正确了。\n",
    "现在有一个疑问，就是为什么照着书上的代码敲，我的就会出现问题，但是书中的代码就没有问题呢？\n",
    "因为在导入numerical_gradient函数的时候，书中直接用储存在别处的函数的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4d9ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4148eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单实现以下两层神经网络的类\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'],self.params['W2']\n",
    "        b1,b2=self.params['b1'],self.params['b2']\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "        return y\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        t=np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t)/float(y.shape[0])\n",
    "        return accuracy\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1']=numerical_gradient_edited(loss_W,self.params['W1'])\n",
    "        grads['b1']=numerical_gradient_edited(loss_W,self.params['b1'])\n",
    "        grads['W2']=numerical_gradient_edited(loss_W,self.params['W2'])\n",
    "        grads['b2']=numerical_gradient_edited(loss_W,self.params['b2'])\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f86e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "net=TwoLayerNet(input_size=784,hidden_size=100,output_size=100)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eeb0f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.rand(100,784)\n",
    "y=net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0fc3d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "x=np.random.rand(100,784)\n",
    "t=np.random.rand(100,10)\n",
    "grads=net.numerical_gradient(x,t)\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dde96f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1197, 64)\n",
      "(600, 64)\n",
      "(1197,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "x_train, x_test = digits.data[600:],digits.data[:600]\n",
    "y_train,y_test = digits.target[600:],digits.target[:600]\n",
    "# 注意训练集和测试集的比例最好 7：3， 8：2\n",
    "print(x_train.shape)\n",
    "print(x_test.shape) \n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7989b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m y_batch = y_train[batch_mask]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#计算梯度\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m grad = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     17\u001b[39m     network.params[key]-=learning_rate*grad[key]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     24\u001b[39m loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m.loss(x,t)\n\u001b[32m     25\u001b[39m grads = {}\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=\u001b[43mnumerical_gradient_edited\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mW1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     28\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mnumerical_gradient_edited\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m      7\u001b[39m original_value = x[idx]\n\u001b[32m      8\u001b[39m x[idx] = original_value + h\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m fxh1=\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m x[idx] = original_value - h\n\u001b[32m     11\u001b[39m fxh2=f(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[39m\u001b[34m(W)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,t):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     grads = {}\n\u001b[32m     26\u001b[39m     grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mTwoLayerNet.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     18\u001b[39m y=\u001b[38;5;28mself\u001b[39m.predict(x)\n\u001b[32m     19\u001b[39m y=np.argmax(y,axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m t=\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m accuracy = np.sum(y==t)/\u001b[38;5;28mfloat\u001b[39m(y.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\vscodeProjects\\introDeepLearning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1341\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1253\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1254\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1338\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1340\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\vscodeProjects\\introDeepLearning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mAxisError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "train_loss_list=[]\n",
    "#超参数\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "\n",
    "network = TwoLayerNet (input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    #计算梯度\n",
    "    grad = network.numerical_gradient(x_batch,y_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "    loss = network.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d8a02",
   "metadata": {},
   "source": [
    "出现这个报错说明在TwoLayerNet这个类里面的loss函数出现了矩阵形状的问题，导致axis=1找不到；\n",
    "现在可以有两种解决办法：1.修改loss方法；2.在输入数据前转化形状即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32626da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,6400) and (64,50) not aligned: 6400 (dim 1) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m y_batch = y_batch.reshape(\u001b[32m1\u001b[39m,-\u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#计算梯度\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m grad = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     20\u001b[39m     network.params[key]-=learning_rate*grad[key]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     24\u001b[39m loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m.loss(x,t)\n\u001b[32m     25\u001b[39m grads = {}\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=\u001b[43mnumerical_gradient_edited\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mW1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     28\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mnumerical_gradient_edited\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m      7\u001b[39m original_value = x[idx]\n\u001b[32m      8\u001b[39m x[idx] = original_value + h\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m fxh1=\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m x[idx] = original_value - h\n\u001b[32m     11\u001b[39m fxh2=f(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[39m\u001b[34m(W)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,t):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     grads = {}\n\u001b[32m     26\u001b[39m     grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mTwoLayerNet.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,t):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     y=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     y=np.argmax(y,axis=\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m     t=np.argmax(t,axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mTwoLayerNet.predict\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     10\u001b[39m W1,W2=\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m],\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m b1,b2=\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m],\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m a1=\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m+b1\n\u001b[32m     13\u001b[39m z1=sigmoid(a1)\n\u001b[32m     14\u001b[39m a2=np.dot(z1,W2)+b2\n",
      "\u001b[31mValueError\u001b[39m: shapes (1,6400) and (64,50) not aligned: 6400 (dim 1) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "train_loss_list=[]\n",
    "#超参数\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "\n",
    "network = TwoLayerNet (input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    #在输入numerical_gradient之前需要修改形状\n",
    "    x_batch = x_batch.reshape(1,-1)\n",
    "    y_batch = y_batch.reshape(1,-1)\n",
    "    #计算梯度\n",
    "    grad = network.numerical_gradient(x_batch,y_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "    loss = network.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d33284f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m y_batch = y_train[batch_mask]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#计算梯度\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m grad = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     17\u001b[39m     network.params[key]-=learning_rate*grad[key]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     24\u001b[39m loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m.loss(x,t)\n\u001b[32m     25\u001b[39m grads = {}\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=\u001b[43mnumerical_gradient_edited\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mW1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     28\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mnumerical_gradient_edited\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m      7\u001b[39m original_value = x[idx]\n\u001b[32m      8\u001b[39m x[idx] = original_value + h\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m fxh1=\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m x[idx] = original_value - h\n\u001b[32m     11\u001b[39m fxh2=f(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[39m\u001b[34m(W)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,t):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     grads = {}\n\u001b[32m     26\u001b[39m     grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mTwoLayerNet.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     18\u001b[39m y=\u001b[38;5;28mself\u001b[39m.predict(x)\n\u001b[32m     19\u001b[39m y=np.argmax(y,axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m t=\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m accuracy = np.sum(y==t)/\u001b[38;5;28mfloat\u001b[39m(y.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\vscodeProjects\\introDeepLearning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1341\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1253\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1254\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1338\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1340\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\vscodeProjects\\introDeepLearning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mAxisError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "train_loss_list=[]\n",
    "#超参数\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "\n",
    "network = TwoLayerNet (input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    #计算梯度\n",
    "    grad = network.numerical_gradient(x_batch,y_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "    loss = network.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2dc6ee",
   "metadata": {},
   "source": [
    "这边给出的报错显示是loss函数里面使用到axis=1的地方，出现问题，所以之前在那里修改x.reshape(1,-1)的思路是错的，要修改的地方直接是\n",
    "loss函数中y的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单实现以下两层神经网络的类\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'],self.params['W2']\n",
    "        b1,b2=self.params['b1'],self.params['b2']\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "        return y\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        t=np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t)/float(y.shape[0])\n",
    "        return accuracy\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1']=numerical_gradient_edited(loss_W,self.params['W1'])\n",
    "        grads['b1']=numerical_gradient_edited(loss_W,self.params['b1'])\n",
    "        grads['W2']=numerical_gradient_edited(loss_W,self.params['W2'])\n",
    "        grads['b2']=numerical_gradient_edited(loss_W,self.params['b2'])\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7ceddd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m y_batch = y_train[batch_mask]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#计算梯度\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m grad = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     17\u001b[39m     network.params[key]-=learning_rate*grad[key]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     27\u001b[39m loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m.loss(x,t)\n\u001b[32m     28\u001b[39m grads = {}\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=\u001b[43mnumerical_gradient_edited\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mW1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     31\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mnumerical_gradient_edited\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m      7\u001b[39m original_value = x[idx]\n\u001b[32m      8\u001b[39m x[idx] = original_value + h\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m fxh1=\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m x[idx] = original_value - h\n\u001b[32m     11\u001b[39m fxh2=f(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[39m\u001b[34m(W)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,t):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     grads = {}\n\u001b[32m     29\u001b[39m     grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=numerical_gradient_edited(loss_W,\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mTwoLayerNet.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     21\u001b[39m     t= t.reshape(\u001b[32m1\u001b[39m,-\u001b[32m1\u001b[39m)\n\u001b[32m     22\u001b[39m y=np.argmax(y,axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m t=\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m accuracy = np.sum(y==t)/\u001b[38;5;28mfloat\u001b[39m(y.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\vscodeProjects\\introDeepLearning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1341\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1253\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1254\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1338\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1340\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\vscodeProjects\\introDeepLearning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mAxisError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "train_loss_list=[]\n",
    "#超参数\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "\n",
    "network = TwoLayerNet (input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    #计算梯度\n",
    "    grad = network.numerical_gradient(x_batch,y_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "    loss = network.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f5309",
   "metadata": {},
   "source": [
    "这边依旧是有错误的原因，需要修改原来TwoLayerNet里面的numerical_gradient_edited方法.\n",
    "解决方法是把原来TwoLayerNet里面的numerical_gradient方法删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "316f101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单实现以下两层神经网络的类\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'],self.params['W2']\n",
    "        b1,b2=self.params['b1'],self.params['b2']\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "        return y\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        t=np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t)/float(y.shape[0])\n",
    "        return accuracy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7bdfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_net(net,x,t):\n",
    "    grads={}\n",
    "    h=1e-4\n",
    "    for key in net.params:\n",
    "        param = net.params[key]\n",
    "        grad = np.zeros_like(param)\n",
    "        it = np.nditer(param,flags=['multi_index'],op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx=it.multi_index\n",
    "            original_val = param[idx]\n",
    "            #f(x+h)\n",
    "            param[idx]=original_val + h\n",
    "            loss1=net.loss(x,t)\n",
    "            #f(x-h)\n",
    "            param[idx]=original_val - h\n",
    "            loss2=net.loss(x,h)\n",
    "            grad[idx]=(loss1-loss2)/(2*h)\n",
    "            #恢复原值\n",
    "            param[idx] = original_val\n",
    "            it.iternext()\n",
    "        grads[key]=grad\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "688927e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (100,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m y_batch = y_train[batch_mask]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#计算梯度\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m grad = \u001b[43mnumerical_gradient_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     17\u001b[39m     network.params[key]-=learning_rate*grad[key]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mnumerical_gradient_net\u001b[39m\u001b[34m(net, x, t)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#f(x+h)\u001b[39;00m\n\u001b[32m     12\u001b[39m param[idx]=original_val + h\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m loss1=\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#f(x-h)\u001b[39;00m\n\u001b[32m     15\u001b[39m param[idx]=original_val - h\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mTwoLayerNet.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,t):\n\u001b[32m     18\u001b[39m     y=\u001b[38;5;28mself\u001b[39m.predict(x)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcross_entropy_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcross_entropy_error\u001b[39m\u001b[34m(y, t)\u001b[39m\n\u001b[32m      5\u001b[39m     y = y.reshape(\u001b[32m1\u001b[39m,y.size)\n\u001b[32m      6\u001b[39m batch_szie=y.shape[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -np.sum(\u001b[43mt\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m+\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m)/batch_size\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (100,) (100,10) "
     ]
    }
   ],
   "source": [
    "train_loss_list=[]\n",
    "#超参数\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "\n",
    "network = TwoLayerNet (input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    #计算梯度\n",
    "    grad = numerical_gradient_net(network,x_batch,y_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "    loss = network.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3510e",
   "metadata": {},
   "source": [
    "现在这个问题又出在cross_entropy_error这个函数上面了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "efe442f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_new(y,t):\n",
    "    delta=1e-7\n",
    "    if t.ndim ==1:\n",
    "        return -np.sum(np.log(y[np.arange(y.shape[0]),t]+delta))/y.shape[0]\n",
    "    elif t.ndim==2:\n",
    "        return -np.sum(t*np.log(y+delta))/y.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "36e10ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单实现以下两层神经网络的类\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'],self.params['W2']\n",
    "        b1,b2=self.params['b1'],self.params['b2']\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "        return y\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        return cross_entropy_error_new(y,t)\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        t=np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t)/float(y.shape[0])\n",
    "        return accuracy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6a1203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_net(net,x,t):\n",
    "    grads={}\n",
    "    h=1e-4\n",
    "    for key in net.params:\n",
    "        param = net.params[key]\n",
    "        grad = np.zeros_like(param)\n",
    "        it = np.nditer(param,flags=['multi_index'],op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx=it.multi_index\n",
    "            original_val = param[idx]\n",
    "            #f(x+h)\n",
    "            param[idx]=original_val + h\n",
    "            loss1=net.loss(x,t)\n",
    "            #f(x-h)\n",
    "            param[idx]=original_val - h\n",
    "            loss2=net.loss(x,t)\n",
    "            grad[idx]=(loss1-loss2)/(2*h)\n",
    "            #恢复原值\n",
    "            param[idx] = original_val\n",
    "            it.iternext()\n",
    "        grads[key]=grad\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7160006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list=[]\n",
    "#超参数\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "\n",
    "network = TwoLayerNet (input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    #计算梯度\n",
    "    grad = numerical_gradient_net(network,x_batch,y_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "    loss = network.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf213391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_list,label='Training Loss',color='blue')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
