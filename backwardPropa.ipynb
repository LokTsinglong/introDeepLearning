{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8241842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        out = x*y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*self.y\n",
    "        dy=dout*self.x\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d836cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num=2\n",
    "tax=1.1\n",
    "\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "price=mul_tax_layer.forward(apple_price,tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf4badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "dprice=1\n",
    "dapple_price,dtax = mul_tax_layer.backward(dprice)\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "print(dapple,dapple_num,dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3653c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x,y):\n",
    "        out=x+y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d2b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "#根据书上138页的计算图写出反向传播的代码\n",
    "apple=100\n",
    "apple_num=2\n",
    "orange=150\n",
    "orange_num=3\n",
    "tax=1.1\n",
    "\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_orange_layer=MulLayer()\n",
    "add_apple_orange_layer=AddLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price=mul_apple_layer.forward(apple,apple_num)\n",
    "orange_price=mul_orange_layer.forward(orange,orange_num)\n",
    "all_price=add_apple_orange_layer.forward(apple_price,orange_price)\n",
    "price = mul_tax_layer.forward(all_price,tax)\n",
    "\n",
    "#backward\n",
    "dprice=1\n",
    "dall_price,dtax=mul_tax_layer.backward(dprice)\n",
    "dapple_price,dorange_price=add_apple_orange_layer.backward(dall_price)\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "dorange,dorange_num=mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num,dapple,dorange,dorange_num,dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9262c7",
   "metadata": {},
   "source": [
    "这边值得注意的一点是：dapple,dapple_num=...。这边的顺序需要和之前forward里面的顺序保持一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7135ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask = (x<=0)\n",
    "        out=x.copy()\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask]=0\n",
    "        dx=dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6676e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([[1.,-0.5],[-2.,3.]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3782d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe373a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self,x):\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*(1.0-self.out)*self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ebe3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.random.rand(2)\n",
    "W=np.random.rand(2,3)\n",
    "B=np.random.rand(3)\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2328fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.dot(X,W)+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ecdfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30807289 0.66669631]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0668f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W=np.array([[0,0,0],[10,10,10]])\n",
    "B=np.array([1,2,3])\n",
    "print(X_dot_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "033e3c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a085b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ad1dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print(dY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfdc2559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY,axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6644bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        out=np.dot(x,self.W)+self.b\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=np.dot(dout,self.W.T)\n",
    "        self.dW=np.dot(self.x.T,dout)\n",
    "        self.db=np.sum(dout,axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a62cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    #exp_a = np.exp(a)\n",
    "    #sum_exp_a = np.sum(exp_a)\n",
    "    #y=exp_a/sum_exp_a\n",
    "    #return y\n",
    "    #旧版本的softmax可能会出现溢出问题\n",
    "    c=np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    return y\n",
    "    # 但是现在这个版本的softmax函数，没有考虑x是一批批输入的\n",
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f952b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#因为从load_digits里面获取得到的标签都不是one-hot格式的，所以导致在计算cross entropy erorr的时候，出现了形状的错误\n",
    "#t的形状是(3,)而不是(3,10)这显然不是我们想要的\n",
    "def cross_entropy_error_new(y,t):\n",
    "    delta=1e-7\n",
    "    batch_size=y.shape[0]\n",
    "    #如果t标签是一维数组而不是one-hot的情况\n",
    "    #也就是load_digits里数据的情况\n",
    "    if t.ndim == 1:\n",
    "        t_onehot = np.zeros_like(y)\n",
    "        t_onehot[np.arange(batch_size),t]=1\n",
    "        t=t_onehot\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size\n",
    "#这个cross_entropy_error_new是要在SoftmaxWithLoss的forward方法里面使用\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4797f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_new(a):\n",
    "    if a.ndim ==2: #说明这个是批量数据\n",
    "        a = a - np.max(a,axis=1,keepdims=True)\n",
    "        exp_a = np.exp(a)\n",
    "        sum_exp_a=np.sum(exp_a,axis=1,keepdims=True)\n",
    "        y=exp_a / sum_exp_a\n",
    "    else:\n",
    "        c=np.max(a) \n",
    "        exp_a=np.exp(a-c) #防止溢出\n",
    "        sum_exp_a = np.sum(exp_a)\n",
    "        y=exp_a/sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "772e45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_f(y,t):\n",
    "    batch_size=y.shape[0]\n",
    "    if t.ndim == 1:\n",
    "        t_onehot = np.zeros_like(y)\n",
    "        t_onehot[np.arange(batch_size),t]=1\n",
    "        t=t_onehot\n",
    "    return (y-t)/batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79c178c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss=None\n",
    "        self.y=None\n",
    "        self.t=None\n",
    "    def forward(self,x,t):\n",
    "        self.t= t\n",
    "        self.y=softmax_new(x)\n",
    "        self.loss=cross_entropy_error_new(self.y,self.t)\n",
    "        return self.loss\n",
    "    def backward(self,dout=1):\n",
    "        #batch_size = self.t.shape[0]\n",
    "        #dx=(self.y-self.t) / batch_size\n",
    "        dx=backward_f(self.y,self.t)\n",
    "        return dx\n",
    "        #r如果没有这个return的话，SoftmaxWithLoss.backward 没有返回值，导致在 TwoLayerNet.gradient() 里，\n",
    "        # 反向传播链条传下去的 dout 变成了 None，最后在 Affine.backward 里做矩阵乘法的时候就报了\n",
    "        # dx=np.dot(dout,self.W.T)\n",
    "        #TypeError: unsupported operand type(s) for *: 'NoneType' and 'float'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d589fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask = (x<=0)\n",
    "        out=x.copy()\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask]=0\n",
    "        dx=dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f04c90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_edited(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=[\"multi_index\"],op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        original_value = x[idx]\n",
    "        x[idx] = original_value + h\n",
    "        fxh1=f(x)\n",
    "        x[idx] = original_value - h\n",
    "        fxh2=f(x)\n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = original_value\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f55959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderedDict:\n",
    "    def __init__(self):\n",
    "        self.keys=[]\n",
    "        self._values_list=[] #这边self的属性不可以和后续的values方法同一个名字，否则会覆盖掉\n",
    "        self._dict={}  #内部字典用于快速查找\n",
    "    \n",
    "    def __setitem__(self,key,value):\n",
    "        if key not in self._dict:\n",
    "            self.keys.append(key)\n",
    "            self._values_list.append(value)\n",
    "        self._dict[key]=value\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        return self._dict[key]\n",
    "    \n",
    "    def values(self):\n",
    "        return self._values_list.copy()\n",
    "    \n",
    "    def __contains__(self,key):\n",
    "        return key in self._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2ea9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        #self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)*np.sqrt(2.0 / input_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        #self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params['W2']=weight_init_std*np.random.randn(hidden_size,output_size)*np.sqrt(1.0 / input_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "\n",
    "        #生成层\n",
    "        self.layers=OrderedDict()\n",
    "        self.layers['Affine1']=Affine(self.params['W1'],self.params['b1'])\n",
    "        self.layers['Relu1']=Relu()\n",
    "        self.layers['Affine2']=Affine(self.params['W2'],self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W : self.loss(x,t)\n",
    "        grads={}\n",
    "        global numerical_gradient_edited #设置为全局变量\n",
    "        grads['W1']=numerical_gradient_edited(loss_W,self.params['W1'])\n",
    "        grads['b1']=numerical_gradient_edited(loss_W,self.params['b1'])\n",
    "        grads['W2']=numerical_gradient_edited(loss_W,self.params['W2'])\n",
    "        grads['b2']=numerical_gradient_edited(loss_W,self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self,x,t):\n",
    "        #forward\n",
    "        self.loss(x,t)\n",
    "\n",
    "        #backward\n",
    "        dout=1\n",
    "        dout=self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads={}\n",
    "        grads['W1']=self.layers['Affine1'].dW\n",
    "        grads['b1']=self.layers['Affine1'].db\n",
    "        grads['W2']=self.layers['Affine2'].dW\n",
    "        grads['b2']=self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1197, 64)\n",
      "(600, 64)\n",
      "(1197,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "#导入数据\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "x_train, x_test = digits.data[600:],digits.data[:600]\n",
    "y_train,y_test = digits.target[600:],digits.target[:600]\n",
    "# 注意训练集和测试集的比例最好 7：3， 8：2\n",
    "print(x_train.shape)\n",
    "print(x_test.shape) \n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 16\n",
    "x_test = x_test / 16\n",
    "#对原始数据做一些归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ca865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0. 10. 15.  2.  0.  0.  0.  0.  7. 16. 16.  6.  0.  0.  0.  0. 12.\n",
      " 13. 12.  9.  0.  0.  0.  0.  8.  9. 13.  7.  0.  0.  0.  0.  0.  0. 16.\n",
      "  5.  0.  0.  0.  0.  0.  6. 15.  1.  0.  0.  0.  0.  0. 16. 14.  4.  5.\n",
      "  8.  3.  0.  0.  8. 16. 16. 16. 16.  9.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0.009435677477183326\n",
      "b1:0.0018695220899930544\n",
      "W2:0.019020241916530383\n",
      "b2:0.06666666678634871\n"
     ]
    }
   ],
   "source": [
    "#误差反向传播的梯度确认gradient_check\n",
    "network=TwoLayerNet(input_size=64,hidden_size=50,output_size=10)\n",
    "x_batch =x_train[:3]\n",
    "y_batch =y_train[:3]\n",
    "grad_numerical= network.numerical_gradient(x_batch,y_batch)\n",
    "grad_backprop=network.gradient(x_batch,y_batch)\n",
    "#求各个权重的绝对误差的平均值\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key]-grad_numerical[key]))\n",
    "    print(key+':'+str(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a68750",
   "metadata": {},
   "source": [
    "值得注意的是书中用load_mnist跑出结果是类似于： b1: 9.704e-13 这种数量级的结果，但是自己跑出来的结果数量级没有这么小，说明是不是数据量太小了，信息量太小了？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e94af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0.012668550031717624\n",
      "b1:0.002576020921188459\n",
      "W2:0.025018302032481853\n",
      "b2:0.09000000012034817\n"
     ]
    }
   ],
   "source": [
    "network2 = TwoLayerNet(input_size=64,hidden_size=50,output_size=10)\n",
    "#\n",
    "x_batch =x_train[:10]\n",
    "y_batch=y_train[:10]\n",
    "grad_numerical= network.numerical_gradient(x_batch,y_batch)\n",
    "grad_backprop=network.gradient(x_batch,y_batch)\n",
    "#求各个权重的绝对误差的平均值\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key]-grad_numerical[key]))\n",
    "    print(key+':'+str(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbf708",
   "metadata": {},
   "source": [
    "也可能是因为数据集本来x的维度就不一样，导致中间计算的不太一样，就比如说，如果采用书中的load_mnist的数据集它的每一个数据样本的维度是784,而我使用的64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14a35110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1010860484544695 0.09833333333333333\n"
     ]
    }
   ],
   "source": [
    "network3 = TwoLayerNet(input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "iters_num=10000\n",
    "train_size = x_train.shape[0] #训练集样本综总数\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    #通过误差反向传播求梯度\n",
    "    grad=network3.gradient(x_batch,y_batch)\n",
    "\n",
    "    #更新\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network3.params[key] -= learning_rate*grad[key]\n",
    "    loss = network3.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i% iter_per_epoch ==0:\n",
    "        train_acc = network3.accuracy(x_train,y_train)\n",
    "        test_acc = network3.accuracy(x_test,y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc,test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a992e9",
   "metadata": {},
   "source": [
    "现在模型的准确只在10%左右，说明如果模型完全不会学习，只是“乱猜”，准确率大概就是 1/10 = 10%。\n",
    "所以你现在的情况说明：模型完全没学到东西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d524b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "(1197,)\n"
     ]
    }
   ],
   "source": [
    "print(np.min(x_train))\n",
    "print(np.max(x_train))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9c6656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 7 4 6 3 1 3 9 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9967112",
   "metadata": {},
   "source": [
    "y_train现在是 整数标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9233333333333333\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9283333333333333\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.925\n",
      "1.0 0.925\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n",
      "1.0 0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "network4 = TwoLayerNet(input_size=64,hidden_size=50,output_size=10)\n",
    "\n",
    "iters_num=10000\n",
    "train_size = x_train.shape[0] #训练集样本综总数\n",
    "batch_size=100\n",
    "learning_rate=0.01 #学习率进行调整了\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "#iter_per_epoch = max(train_size / batch_size, 1) \n",
    "iter_per_epoch = max(train_size // batch_size, 1)  # 用整除，返回 int\n",
    "#原来用 / 得到的是浮点数，i % 浮点数 == 0 基本只会在 i=0 成立，导致看起来“没变化”。\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    #通过误差反向传播求梯度\n",
    "    grad=network3.gradient(x_batch,y_batch)\n",
    "\n",
    "    #更新\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network3.params[key] -= learning_rate*grad[key]\n",
    "    loss = network3.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    \n",
    "    if i% iter_per_epoch ==0:\n",
    "        train_acc = network3.accuracy(x_train,y_train)\n",
    "        test_acc = network3.accuracy(x_test,y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc,test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b10a01",
   "metadata": {},
   "source": [
    "这一次训练效果明显变好了，做的修改是： 1. 在初始化W参数的时候采用了he/Xavier初始化 2. learning rate =0.1 -> 0.01 3. iter_per_epoch 那里修改为整除"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
