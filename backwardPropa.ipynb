{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8241842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        out = x*y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*self.y\n",
    "        dy=dout*self.x\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d836cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num=2\n",
    "tax=1.1\n",
    "\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "price=mul_tax_layer.forward(apple_price,tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf4badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "dprice=1\n",
    "dapple_price,dtax = mul_tax_layer.backward(dprice)\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "print(dapple,dapple_num,dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3653c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x,y):\n",
    "        out=x+y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d2b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "#根据书上138页的计算图写出反向传播的代码\n",
    "apple=100\n",
    "apple_num=2\n",
    "orange=150\n",
    "orange_num=3\n",
    "tax=1.1\n",
    "\n",
    "mul_apple_layer=MulLayer()\n",
    "mul_orange_layer=MulLayer()\n",
    "add_apple_orange_layer=AddLayer()\n",
    "mul_tax_layer=MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price=mul_apple_layer.forward(apple,apple_num)\n",
    "orange_price=mul_orange_layer.forward(orange,orange_num)\n",
    "all_price=add_apple_orange_layer.forward(apple_price,orange_price)\n",
    "price = mul_tax_layer.forward(all_price,tax)\n",
    "\n",
    "#backward\n",
    "dprice=1\n",
    "dall_price,dtax=mul_tax_layer.backward(dprice)\n",
    "dapple_price,dorange_price=add_apple_orange_layer.backward(dall_price)\n",
    "dapple,dapple_num=mul_apple_layer.backward(dapple_price)\n",
    "dorange,dorange_num=mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num,dapple,dorange,dorange_num,dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9262c7",
   "metadata": {},
   "source": [
    "这边值得注意的一点是：dapple,dapple_num=...。这边的顺序需要和之前forward里面的顺序保持一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7135ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask = (x<=0)\n",
    "        out=x.copy()\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask]=0\n",
    "        dx=dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6676e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([[1.,-0.5],[-2.,3.]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3782d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe373a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self,x):\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=dout*(1.0-self.out)*self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ebe3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.random.rand(2)\n",
    "W=np.random.rand(2,3)\n",
    "B=np.random.rand(3)\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2328fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.dot(X,W)+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ecdfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30807289 0.66669631]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0668f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W=np.array([[0,0,0],[10,10,10]])\n",
    "B=np.array([1,2,3])\n",
    "print(X_dot_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "033e3c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a085b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ad1dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print(dY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfdc2559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY,axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6644bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        out=np.dot(x,self.W)+self.b\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx=np.dot(dout,self.W.T)\n",
    "        self.dW=np.dot(self.x.T,dout)\n",
    "        self.db=np.sum(dout,axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6a62cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    return y\n",
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f952b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#因为从load_digits里面获取得到的标签都不是one-hot格式的，所以导致在计算cross entropy erorr的时候，出现了形状的错误\n",
    "#t的形状是(3,)而不是(3,10)这显然不是我们想要的\n",
    "def cross_entropy_error_new(y,t):\n",
    "    delta=1e-7\n",
    "    batch_size=y.shape[0]\n",
    "    #如果t标签是一维数组而不是one-hot的情况\n",
    "    #也就是load_digits里数据的情况\n",
    "    if t.ndim == 1:\n",
    "        t_onehot = np.zeros_like(y)\n",
    "        t_onehot[np.arange(batch_size),t]=1\n",
    "        t=t_onehot\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size\n",
    "#这个cross_entropy_error_new是要在SoftmaxWithLoss的forward方法里面使用\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "772e45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_f(y,t):\n",
    "    batch_size=y.shape[0]\n",
    "    if t.ndim == 1:\n",
    "        t_onehot = np.zeros_like(y)\n",
    "        t_onehot[np.arange(batch_size),t]=1\n",
    "        t=t_onehot\n",
    "    return (y-t)/batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79c178c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss=None\n",
    "        self.y=None\n",
    "        self.t=None\n",
    "    def forward(self,x,t):\n",
    "        self.t= t\n",
    "        self.y=softmax(x)\n",
    "        self.loss=cross_entropy_error_new(self.y,self.t)\n",
    "        return self.loss\n",
    "    def backward(self,dout=1):\n",
    "        #batch_size = self.t.shape[0]\n",
    "        #dx=(self.y-self.t) / batch_size\n",
    "        dx=backward_f(self.y,self.t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d589fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask = (x<=0)\n",
    "        out=x.copy()\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask]=0\n",
    "        dx=dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f04c90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_edited(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=[\"multi_index\"],op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        original_value = x[idx]\n",
    "        x[idx] = original_value + h\n",
    "        fxh1=f(x)\n",
    "        x[idx] = original_value - h\n",
    "        fxh2=f(x)\n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = original_value\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f55959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderedDict:\n",
    "    def __init__(self):\n",
    "        self.keys=[]\n",
    "        self._values_list=[] #这边self的属性不可以和后续的values方法同一个名字，否则会覆盖掉\n",
    "        self._dict={}  #内部字典用于快速查找\n",
    "    \n",
    "    def __setitem__(self,key,value):\n",
    "        if key not in self._dict:\n",
    "            self.keys.append(key)\n",
    "            self._values_list.append(value)\n",
    "        self._dict[key]=value\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        return self._dict[key]\n",
    "    \n",
    "    def values(self):\n",
    "        return self._values_list.copy()\n",
    "    \n",
    "    def __contains__(self,key):\n",
    "        return key in self._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2ea9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*np.random.rand(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "\n",
    "        #生成层\n",
    "        self.layers=OrderedDict()\n",
    "        self.layers['Affine1']=Affine(self.params['W1'],self.params['b1'])\n",
    "        self.layers['Relu1']=Relu()\n",
    "        self.layers['Affine2']=Affine(self.params['W2'],self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W : self.loss(x,t)\n",
    "        grads={}\n",
    "        global numerical_gradient_edited #设置为全局变量\n",
    "        grads['W1']=numerical_gradient_edited(loss_W,self.params['W1'])\n",
    "        grads['b1']=numerical_gradient_edited(loss_W,self.params['b1'])\n",
    "        grads['W2']=numerical_gradient_edited(loss_W,self.params['W2'])\n",
    "        grads['b2']=numerical_gradient_edited(loss_W,self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self,x,t):\n",
    "        #forward\n",
    "        self.loss(x,t)\n",
    "\n",
    "        #backward\n",
    "        dout=1\n",
    "        dout=self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads={}\n",
    "        grads['W1']=self.layers['Affine1'].dW\n",
    "        grads['b1']=self.layers['Affine1'].db\n",
    "        grads['W2']=self.layers['Affine2'].dW\n",
    "        grads['b2']=self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a499ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1197, 64)\n",
      "(600, 64)\n",
      "(1197,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "#导入数据\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "x_train, x_test = digits.data[600:],digits.data[:600]\n",
    "y_train,y_test = digits.target[600:],digits.target[:600]\n",
    "# 注意训练集和测试集的比例最好 7：3， 8：2\n",
    "print(x_train.shape)\n",
    "print(x_test.shape) \n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5650d823",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m y_batch=y_train[:\u001b[32m3\u001b[39m]\n\u001b[32m      5\u001b[39m grad_numerical= network.numerical_gradient(x_batch,y_batch)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m grad_backprop=\u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#求各个权重的绝对误差的平均值\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m grad_numerical.keys():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mTwoLayerNet.gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     53\u001b[39m layers.reverse()\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     dout = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m grads={}\n\u001b[32m     58\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m]=\u001b[38;5;28mself\u001b[39m.layers[\u001b[33m'\u001b[39m\u001b[33mAffine1\u001b[39m\u001b[33m'\u001b[39m].dW\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mAffine.backward\u001b[39m\u001b[34m(self, dout)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m,dout):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     dx=\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m.dW=np.dot(\u001b[38;5;28mself\u001b[39m.x.T,dout)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m.db=np.sum(dout,axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "#误差反向传播的梯度确认\n",
    "network=TwoLayerNet(input_size=64,hidden_size=50,output_size=10)\n",
    "x_batch =x_train[:3]\n",
    "y_batch=y_train[:3]\n",
    "grad_numerical= network.numerical_gradient(x_batch,y_batch)\n",
    "grad_backprop=network.gradient(x_batch,y_batch)\n",
    "#求各个权重的绝对误差的平均值\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key]-grad_numerical[key]))\n",
    "    print(key+':'+str(diff))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
